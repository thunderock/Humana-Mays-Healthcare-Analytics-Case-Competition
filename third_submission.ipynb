{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "942aabd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bh_ncdm_ind</th>\n",
       "      <th>auth_3mth_post_acute_inf</th>\n",
       "      <th>rx_maint_net_paid_pmpm_cost_t_9-6-3m_b4</th>\n",
       "      <th>ccsp_065_pmpm_ct</th>\n",
       "      <th>auth_3mth_acute_vco</th>\n",
       "      <th>rx_gpi2_72_pmpm_ct_6to9m_b4</th>\n",
       "      <th>auth_3mth_post_acute_men</th>\n",
       "      <th>rej_total_physician_office_visit_ct_pmpm_0to3m_b4</th>\n",
       "      <th>total_physician_office_net_paid_pmpm_cost_t_9-6-3m_b4</th>\n",
       "      <th>bh_ip_snf_net_paid_pmpm_cost_0to3m_b4</th>\n",
       "      <th>...</th>\n",
       "      <th>rx_nonmaint_mbr_resp_pmpm_cost</th>\n",
       "      <th>credit_bal_nonmtgcredit_60dpd</th>\n",
       "      <th>atlas_ownhomepct</th>\n",
       "      <th>rx_overall_mbr_resp_pmpm_cost</th>\n",
       "      <th>atlas_redemp_snaps16</th>\n",
       "      <th>atlas_netmigrationrate1016</th>\n",
       "      <th>atlas_percapitainc</th>\n",
       "      <th>phy_em_px_pct</th>\n",
       "      <th>rx_generic_mbr_resp_pmpm_cost_0to3m_b4</th>\n",
       "      <th>covid_vaccination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.159828</td>\n",
       "      <td>0.748533</td>\n",
       "      <td>0.008597</td>\n",
       "      <td>0.184427</td>\n",
       "      <td>0.434299</td>\n",
       "      <td>0.351813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008956</td>\n",
       "      <td>0.197643</td>\n",
       "      <td>0.677332</td>\n",
       "      <td>0.016252</td>\n",
       "      <td>0.282557</td>\n",
       "      <td>0.436958</td>\n",
       "      <td>0.382757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008518</td>\n",
       "      <td>0.305488</td>\n",
       "      <td>0.677281</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>0.285884</td>\n",
       "      <td>0.362410</td>\n",
       "      <td>0.363665</td>\n",
       "      <td>0.130694</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269741</td>\n",
       "      <td>0.463229</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>0.366044</td>\n",
       "      <td>0.303851</td>\n",
       "      <td>0.300465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090292</td>\n",
       "      <td>0.456923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249197</td>\n",
       "      <td>0.384314</td>\n",
       "      <td>0.536348</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 331 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bh_ncdm_ind  auth_3mth_post_acute_inf  \\\n",
       "0            0                         0   \n",
       "1            0                         0   \n",
       "2            0                         0   \n",
       "3            0                         0   \n",
       "4            0                         0   \n",
       "\n",
       "   rx_maint_net_paid_pmpm_cost_t_9-6-3m_b4  ccsp_065_pmpm_ct  \\\n",
       "0                                        1                 1   \n",
       "1                                        8                 1   \n",
       "2                                        0                 1   \n",
       "3                                        0                 1   \n",
       "4                                        4                 1   \n",
       "\n",
       "   auth_3mth_acute_vco  rx_gpi2_72_pmpm_ct_6to9m_b4  auth_3mth_post_acute_men  \\\n",
       "0                    1                            1                         0   \n",
       "1                    1                            1                         0   \n",
       "2                    1                            1                         0   \n",
       "3                    1                            2                         0   \n",
       "4                    1                            1                         0   \n",
       "\n",
       "   rej_total_physician_office_visit_ct_pmpm_0to3m_b4  \\\n",
       "0                                                  1   \n",
       "1                                                  1   \n",
       "2                                                  1   \n",
       "3                                                  1   \n",
       "4                                                  1   \n",
       "\n",
       "   total_physician_office_net_paid_pmpm_cost_t_9-6-3m_b4  \\\n",
       "0                                                  9       \n",
       "1                                                  9       \n",
       "2                                                  1       \n",
       "3                                                  9       \n",
       "4                                                  9       \n",
       "\n",
       "   bh_ip_snf_net_paid_pmpm_cost_0to3m_b4  ...  rx_nonmaint_mbr_resp_pmpm_cost  \\\n",
       "0                                      1  ...                        0.003288   \n",
       "1                                      1  ...                        0.008956   \n",
       "2                                      1  ...                        0.008518   \n",
       "3                                      1  ...                        0.000000   \n",
       "4                                      1  ...                        0.000000   \n",
       "\n",
       "   credit_bal_nonmtgcredit_60dpd  atlas_ownhomepct  \\\n",
       "0                       0.159828          0.748533   \n",
       "1                       0.197643          0.677332   \n",
       "2                       0.305488          0.677281   \n",
       "3                       0.269741          0.463229   \n",
       "4                       0.090292          0.456923   \n",
       "\n",
       "   rx_overall_mbr_resp_pmpm_cost  atlas_redemp_snaps16  \\\n",
       "0                       0.008597              0.184427   \n",
       "1                       0.016252              0.282557   \n",
       "2                       0.014049              0.285884   \n",
       "3                       0.003586              0.366044   \n",
       "4                       0.000000              0.249197   \n",
       "\n",
       "   atlas_netmigrationrate1016  atlas_percapitainc  phy_em_px_pct  \\\n",
       "0                    0.434299            0.351813       0.000000   \n",
       "1                    0.436958            0.382757       0.000000   \n",
       "2                    0.362410            0.363665       0.130694   \n",
       "3                    0.303851            0.300465       0.000000   \n",
       "4                    0.384314            0.536348       0.500000   \n",
       "\n",
       "   rx_generic_mbr_resp_pmpm_cost_0to3m_b4  covid_vaccination  \n",
       "0                                0.018443                  0  \n",
       "1                                0.059102                  0  \n",
       "2                                0.010014                  0  \n",
       "3                                0.012345                  0  \n",
       "4                                0.000000                  0  \n",
       "\n",
       "[5 rows x 331 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
    "import gc\n",
    "gc.enable()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "file = \"dataset/transformed_dataset.csv\"\n",
    "reg_cols = ['atlas_pct_diabetes_adults13',\n",
    " 'atlas_pct_wic15',\n",
    " 'total_physician_office_net_paid_pmpm_cost_9to12m_b4',\n",
    " 'atlas_pct_laccess_hisp15',\n",
    " 'atlas_pct_fmrkt_frveg16',\n",
    " 'credit_hh_nonmtgcredit_60dpd',\n",
    " 'atlas_dirsales_farms12',\n",
    " 'rx_nonmaint_pmpm_ct',\n",
    " 'zip_cd',\n",
    " 'atlas_pct_laccess_white15',\n",
    " 'credit_hh_bankcard_severederog',\n",
    " 'atlas_pct_fmrkt_credit16',\n",
    " 'credit_bal_autofinance_new',\n",
    " 'rej_days_since_last_clm',\n",
    " 'rx_generic_pmpm_ct_0to3m_b4',\n",
    " 'rwjf_social_associate_rate',\n",
    " 'med_physician_office_ds_clm_6to9m_b4',\n",
    " 'atlas_totalocchu',\n",
    " 'atlas_veg_acrespth12',\n",
    " 'atlas_pct_loclsale12',\n",
    " 'atlas_pct_fmrkt_anmlprod16',\n",
    " 'atlas_freshveg_farms12',\n",
    " 'rwjf_resident_seg_black_inx',\n",
    " 'atlas_pct_loclfarm12',\n",
    " 'total_outpatient_mbr_resp_pmpm_cost_6to9m_b4',\n",
    " 'atlas_berry_acrespth12',\n",
    " 'rx_maint_pmpm_ct_9to12m_b4',\n",
    " 'rx_tier_2_pmpm_ct',\n",
    " 'atlas_agritrsm_rct12',\n",
    " 'atlas_pct_laccess_snap15',\n",
    " 'atlas_deep_pov_all',\n",
    " 'ccsp_227_pct',\n",
    " 'bh_outpatient_net_paid_pmpm_cost',\n",
    " 'atlas_veg_farms12',\n",
    " 'rx_hum_16_pmpm_ct',\n",
    " 'cms_risk_adjustment_factor_a_amt',\n",
    " 'atlas_recfac14',\n",
    " 'total_physician_office_copay_pmpm_cost',\n",
    " 'atlas_pc_fsrsales12',\n",
    " 'atlas_pct_fmrkt_baked16',\n",
    " 'atlas_net_international_migration_rate',\n",
    " 'rx_maint_mbr_resp_pmpm_cost_6to9m_b4',\n",
    " 'rx_generic_pmpm_cost_6to9m_b4',\n",
    " 'rx_gpi2_49_pmpm_cost_0to3m_b4',\n",
    " 'atlas_pct_sbp15',\n",
    " 'atlas_pct_laccess_child15',\n",
    " 'met_obe_diag_pct',\n",
    " 'atlas_orchard_acrespth12',\n",
    " 'atlas_pct_laccess_hhnv15',\n",
    " 'cnt_cp_webstatement_pmpm_ct',\n",
    " 'atlas_pct_laccess_lowi15',\n",
    " 'rx_gpi2_02_pmpm_cost',\n",
    " 'cms_partd_ra_factor_amt',\n",
    " 'atlas_pct_free_lunch14',\n",
    " 'rx_tier_2_pmpm_ct_3to6m_b4',\n",
    " 'cons_chva',\n",
    " 'atlas_pct_fmrkt_wiccash16',\n",
    " 'rx_overall_net_paid_pmpm_cost_6to9m_b4',\n",
    " 'total_med_allowed_pmpm_cost_9to12m_b4',\n",
    " 'bh_physician_office_copay_pmpm_cost_6to9m_b4',\n",
    " 'atlas_pct_snap16',\n",
    " 'atlas_ghveg_sqftpth12',\n",
    " 'atlas_pc_dirsales12',\n",
    " 'atlas_pct_reduced_lunch14',\n",
    " 'ccsp_236_pct',\n",
    " 'atlas_deep_pov_children',\n",
    " 'atlas_pct_sfsp15',\n",
    " 'rwjf_air_pollute_density',\n",
    " 'rx_generic_pmpm_cost',\n",
    " 'cms_tot_partd_payment_amt',\n",
    " 'cons_nwperadult',\n",
    " 'rx_days_since_last_script',\n",
    " 'atlas_pct_laccess_nhasian15',\n",
    " 'rx_nonbh_mbr_resp_pmpm_cost_6to9m_b4',\n",
    " 'rx_days_since_last_script_6to9m_b4',\n",
    " 'atlas_pct_obese_adults13',\n",
    " 'credit_bal_consumerfinance',\n",
    " 'atlas_pct_fmrkt_wic16',\n",
    " 'atlas_orchard_farms12',\n",
    " 'atlas_berry_farms12',\n",
    " 'atlas_pct_laccess_multir15',\n",
    " 'rx_bh_mbr_resp_pmpm_cost_9to12m_b4',\n",
    " 'atlas_pc_wic_redemp12',\n",
    " 'rwjf_mv_deaths_rate',\n",
    " 'atlas_povertyunder18pct',\n",
    " 'rx_gpi2_72_pmpm_cost_6to9m_b4',\n",
    " 'atlas_pct_fmrkt_snap16',\n",
    " 'atlas_medhhinc',\n",
    " 'rx_nonbh_net_paid_pmpm_cost',\n",
    " 'credit_bal_bankcard_severederog',\n",
    " 'bh_ip_snf_net_paid_pmpm_cost',\n",
    " 'atlas_pc_snapben15',\n",
    " 'rx_nonbh_pmpm_ct_0to3m_b4',\n",
    " 'rx_overall_mbr_resp_pmpm_cost_0to3m_b4',\n",
    " 'auth_3mth_post_acute_mean_los',\n",
    " 'rx_branded_mbr_resp_pmpm_cost',\n",
    " 'rx_tier_1_pmpm_ct_0to3m_b4',\n",
    " 'bh_ncdm_pct',\n",
    " 'atlas_naturalchangerate1016',\n",
    " 'rx_mail_mbr_resp_pmpm_cost_0to3m_b4',\n",
    " 'credit_bal_autobank',\n",
    " 'rx_nonotc_dist_gpi6_pmpm_ct',\n",
    " 'cons_cgqs',\n",
    " 'rx_overall_gpi_pmpm_ct_0to3m_b4',\n",
    " 'credit_hh_bankcardcredit_60dpd',\n",
    " 'rx_gpi2_01_pmpm_cost_0to3m_b4',\n",
    " 'cci_dia_m_pmpm_ct',\n",
    " 'atlas_pct_nslp15',\n",
    " 'mcc_end_pct',\n",
    " 'atlas_pct_laccess_black15',\n",
    " 'credit_bal_mtgcredit_new',\n",
    " 'credit_hh_1stmtgcredit',\n",
    " 'cons_chmi',\n",
    " 'rwjf_income_inequ_ratio',\n",
    " 'atlas_pct_laccess_pop15',\n",
    " 'atlas_pc_ffrsales12',\n",
    " 'atlas_hh65plusalonepct',\n",
    " 'atlas_pct_fmrkt_sfmnp16',\n",
    " 'auth_3mth_acute_mean_los',\n",
    " 'rx_hum_28_pmpm_cost',\n",
    " 'atlas_pct_laccess_nhna15',\n",
    " 'atlas_povertyallagespct',\n",
    " 'rx_nonbh_mbr_resp_pmpm_cost',\n",
    " 'rx_nonmaint_mbr_resp_pmpm_cost_9to12m_b4',\n",
    " 'atlas_pct_fmrkt_otherfood16',\n",
    " 'lab_dist_loinc_pmpm_ct',\n",
    " 'rx_generic_mbr_resp_pmpm_cost',\n",
    " 'atlas_pct_laccess_seniors15',\n",
    " 'atlas_pct_cacfp15',\n",
    " 'total_outpatient_allowed_pmpm_cost_6to9m_b4',\n",
    " 'rx_nonmaint_mbr_resp_pmpm_cost',\n",
    " 'credit_bal_nonmtgcredit_60dpd',\n",
    " 'atlas_ownhomepct',\n",
    " 'rx_overall_mbr_resp_pmpm_cost',\n",
    " 'atlas_redemp_snaps16',\n",
    " 'atlas_netmigrationrate1016',\n",
    " 'atlas_percapitainc',\n",
    " 'phy_em_px_pct',\n",
    " 'rx_generic_mbr_resp_pmpm_cost_0to3m_b4']\n",
    "\n",
    "cat_cols = ['bh_ncdm_ind',\n",
    " 'auth_3mth_post_acute_inf',\n",
    " 'rx_maint_net_paid_pmpm_cost_t_9-6-3m_b4',\n",
    " 'ccsp_065_pmpm_ct',\n",
    " 'auth_3mth_acute_vco',\n",
    " 'rx_gpi2_72_pmpm_ct_6to9m_b4',\n",
    " 'auth_3mth_post_acute_men',\n",
    " 'rej_total_physician_office_visit_ct_pmpm_0to3m_b4',\n",
    " 'total_physician_office_net_paid_pmpm_cost_t_9-6-3m_b4',\n",
    " 'bh_ip_snf_net_paid_pmpm_cost_0to3m_b4',\n",
    " 'mcc_ano_pmpm_ct_t_9-6-3m_b4',\n",
    " 'atlas_type_2015_update',\n",
    " 'atlas_retirement_destination_2015_upda',\n",
    " 'auth_3mth_post_acute_sns',\n",
    " 'atlas_hiamenity',\n",
    " 'cons_ltmedicr',\n",
    " 'auth_3mth_acute_ccs_086',\n",
    " 'total_physician_office_mbr_resp_pmpm_cost_t_9-6-3m_b4',\n",
    " 'auth_3mth_acute_cir',\n",
    " 'atlas_csa12',\n",
    " 'total_med_net_paid_pmpm_cost_t_6-3-0m_b4',\n",
    " 'cons_n2pwh',\n",
    " 'auth_3mth_snf_post_hsp',\n",
    " 'auth_3mth_post_acute_inj',\n",
    " 'med_outpatient_mbr_resp_pmpm_cost_t_9-6-3m_b4',\n",
    " 'rx_gpi2_56_dist_gpi6_pmpm_ct_3to6m_b4',\n",
    " 'atlas_low_employment_2015_update',\n",
    " 'auth_3mth_acute_inf',\n",
    " 'lab_albumin_loinc_pmpm_ct',\n",
    " 'rx_gpi2_17_pmpm_cost_t_12-9-6m_b4',\n",
    " 'cons_rxadhs',\n",
    " 'cons_mobplus',\n",
    " 'atlas_foodinsec_child_03_11',\n",
    " 'lang_spoken_cd',\n",
    " 'bh_ip_snf_mbr_resp_pmpm_cost_9to12m_b4',\n",
    " 'auth_3mth_post_acute_gus',\n",
    " 'auth_3mth_acute_cad',\n",
    " 'rx_maint_pmpm_ct_t_6-3-0m_b4',\n",
    " 'auth_3mth_acute_ccs_044',\n",
    " 'cons_hxmioc',\n",
    " 'med_outpatient_visit_ct_pmpm_t_12-9-6m_b4',\n",
    " 'med_physician_office_allowed_pmpm_cost_t_9-6-3m_b4',\n",
    " 'auth_3mth_acute_res',\n",
    " 'auth_3mth_acute_chf',\n",
    " 'auth_3mth_acute_ccs_030',\n",
    " 'auth_3mth_dc_hospice',\n",
    " 'auth_3mth_acute_neo',\n",
    " 'atlas_type_2015_recreation_no',\n",
    " 'hum_region',\n",
    " 'atlas_ghveg_farms12',\n",
    " 'rx_maint_net_paid_pmpm_cost_t_12-9-6m_b4',\n",
    " 'auth_3mth_acute_ccs_048',\n",
    " 'rx_overall_gpi_pmpm_ct_t_6-3-0m_b4',\n",
    " 'rx_overall_gpi_pmpm_ct_t_12-9-6m_b4',\n",
    " 'rx_nonbh_pmpm_ct_t_9-6-3m_b4',\n",
    " 'mcc_chf_pmpm_ct_t_9-6-3m_b4',\n",
    " 'auth_3mth_post_acute_chf',\n",
    " 'auth_3mth_psychic',\n",
    " 'rx_nonotc_pmpm_cost_t_6-3-0m_b4',\n",
    " 'auth_3mth_acute_end',\n",
    " 'atlas_low_education_2015_update',\n",
    " 'src_div_id',\n",
    " 'auth_3mth_bh_acute',\n",
    " 'auth_3mth_acute_ccs_067',\n",
    " 'atlas_type_2015_mining_no',\n",
    " 'cons_n2pmr',\n",
    " 'rx_mail_net_paid_pmpm_cost_t_6-3-0m_b4',\n",
    " 'rej_med_er_net_paid_pmpm_cost_t_9-6-3m_b4',\n",
    " 'med_outpatient_deduct_pmpm_cost_t_9-6-3m_b4',\n",
    " 'rej_med_ip_snf_coins_pmpm_cost_t_9-6-3m_b4',\n",
    " 'rx_generic_dist_gpi6_pmpm_ct_t_9-6-3m_b4',\n",
    " 'auth_3mth_dc_home',\n",
    " 'auth_3mth_acute_bld',\n",
    " 'auth_3mth_acute_ner',\n",
    " 'oontwk_mbr_resp_pmpm_cost_t_6-3-0m_b4',\n",
    " 'rx_gpi2_90_dist_gpi6_pmpm_ct_9to12m_b4',\n",
    " 'atlas_foodhub16',\n",
    " 'rx_maint_pmpm_cost_t_6-3-0m_b4',\n",
    " 'auth_3mth_post_acute_ben',\n",
    " 'est_age',\n",
    " 'auth_3mth_post_acute_cer',\n",
    " 'auth_3mth_acute_ccs_153',\n",
    " 'auth_3mth_acute_dig',\n",
    " 'total_ip_maternity_net_paid_pmpm_cost_t_12-9-6m_b4',\n",
    " 'auth_3mth_post_acute_cad',\n",
    " 'rx_bh_pmpm_ct_0to3m_b4',\n",
    " 'rx_nonmail_dist_gpi6_pmpm_ct_t_9-6-3m_b4',\n",
    " 'atlas_persistentchildpoverty_1980_2011',\n",
    " 'atlas_slhouse12',\n",
    " 'atlas_population_loss_2015_update',\n",
    " 'auth_3mth_acute_ccs_094',\n",
    " 'auth_3mth_post_acute_ner',\n",
    " 'auth_3mth_acute_ccs_227',\n",
    " 'rx_overall_dist_gpi6_pmpm_ct_t_6-3-0m_b4',\n",
    " 'auth_3mth_acute_trm',\n",
    " 'auth_3mth_post_acute',\n",
    " 'auth_3mth_acute_dia',\n",
    " 'auth_3mth_acute_ccs_043',\n",
    " 'rx_overall_mbr_resp_pmpm_cost_t_6-3-0m_b4',\n",
    " 'cms_orig_reas_entitle_cd',\n",
    " 'auth_3mth_post_acute_end',\n",
    " 'auth_3mth_acute_can',\n",
    " 'auth_3mth_acute_ccs_172',\n",
    " 'auth_3mth_dc_home_health',\n",
    " 'atlas_hipov_1115',\n",
    " 'rx_phar_cat_cvs_pmpm_ct_t_9-6-3m_b4',\n",
    " 'rx_gpi2_62_pmpm_cost_t_9-6-3m_b4',\n",
    " 'cons_n2phi',\n",
    " 'auth_3mth_post_acute_hdz',\n",
    " 'auth_3mth_bh_acute_mean_los',\n",
    " 'auth_3mth_post_acute_dig',\n",
    " 'auth_3mth_transplant',\n",
    " 'rx_mail_mbr_resp_pmpm_cost_t_9-6-3m_b4',\n",
    " 'auth_3mth_acute_sns',\n",
    " 'auth_3mth_post_acute_vco',\n",
    " 'auth_3mth_home',\n",
    " 'rx_nonbh_net_paid_pmpm_cost_t_6-3-0m_b4',\n",
    " 'auth_3mth_post_acute_ckd',\n",
    " 'rx_gpi2_34_dist_gpi6_pmpm_ct',\n",
    " 'rx_gpi2_33_pmpm_ct_0to3m_b4',\n",
    " 'auth_3mth_dc_ltac',\n",
    " 'cons_estinv30_rc',\n",
    " 'rx_phar_cat_humana_pmpm_ct_t_9-6-3m_b4',\n",
    " 'auth_3mth_acute_men',\n",
    " 'auth_3mth_dc_snf',\n",
    " 'cons_hhcomp',\n",
    " 'bh_ip_snf_mbr_resp_pmpm_cost_6to9m_b4',\n",
    " 'auth_3mth_acute_inj',\n",
    " 'total_physician_office_visit_ct_pmpm_t_6-3-0m_b4',\n",
    " 'mabh_seg',\n",
    " 'auth_3mth_post_acute_res',\n",
    " 'auth_3mth_bh_acute_men',\n",
    " 'auth_3mth_acute_hdz',\n",
    " 'hedis_dia_hba1c_ge9',\n",
    " 'auth_3mth_post_acute_trm',\n",
    " 'auth_3mth_hospice',\n",
    " 'rx_gpi2_39_pmpm_cost_t_6-3-0m_b4',\n",
    " 'atlas_vlfoodsec_13_15',\n",
    " 'auth_3mth_dc_acute_rehab',\n",
    " 'rx_generic_pmpm_cost_t_6-3-0m_b4',\n",
    " 'auth_3mth_acute_ccs_154',\n",
    " 'cons_rxmaint',\n",
    " 'total_bh_copay_pmpm_cost_t_9-6-3m_b4',\n",
    " 'rx_nonmaint_dist_gpi6_pmpm_ct_t_12-9-6m_b4',\n",
    " 'rej_med_outpatient_visit_ct_pmpm_t_6-3-0m_b4',\n",
    " 'cons_rxadhm',\n",
    " 'auth_3mth_acute_mus',\n",
    " 'rx_nonbh_pmpm_cost_t_9-6-3m_b4',\n",
    " 'rx_days_since_last_script_0to3m_b4',\n",
    " 'auth_3mth_post_acute_cir',\n",
    " 'auth_3mth_post_acute_dia',\n",
    " 'auth_3mth_post_er',\n",
    " 'auth_3mth_dc_no_ref',\n",
    " 'bh_ip_snf_mbr_resp_pmpm_cost_3to6m_b4',\n",
    " 'auth_3mth_acute',\n",
    " 'rx_branded_pmpm_ct_t_6-3-0m_b4',\n",
    " 'atlas_farm_to_school13',\n",
    " 'auth_3mth_acute_cer',\n",
    " 'med_ambulance_coins_pmpm_cost_t_9-6-3m_b4',\n",
    " 'auth_3mth_acute_gus',\n",
    " 'rx_gpi4_6110_pmpm_ct',\n",
    " 'cons_hxwearbl',\n",
    " 'auth_3mth_ltac',\n",
    " 'auth_3mth_acute_ckd',\n",
    " 'bh_ip_snf_net_paid_pmpm_cost_6to9m_b4',\n",
    " 'sex_cd',\n",
    " 'days_since_last_clm_0to3m_b4',\n",
    " 'atlas_perpov_1980_0711',\n",
    " 'auth_3mth_post_acute_mus',\n",
    " 'auth_3mth_non_er',\n",
    " 'bh_ncal_ind',\n",
    " 'auth_3mth_facility',\n",
    " 'atlas_foodinsec_13_15',\n",
    " 'auth_3mth_dc_left_ama',\n",
    " 'race_cd',\n",
    " 'bh_ip_snf_admit_days_pmpm_t_9-6-3m_b4',\n",
    " 'auth_3mth_dc_other',\n",
    " 'cons_stlnindx',\n",
    " 'auth_3mth_acute_skn',\n",
    " 'total_allowed_pmpm_cost_t_9-6-3m_b4',\n",
    " 'auth_3mth_rehab',\n",
    " 'bh_urgent_care_copay_pmpm_cost_t_12-9-6m_b4',\n",
    " 'auth_3mth_dc_custodial',\n",
    " 'auth_3mth_snf_direct',\n",
    " 'auth_3mth_acute_ccs_042',\n",
    " 'bh_ip_snf_net_paid_pmpm_cost_9to12m_b4',\n",
    " 'bh_ip_snf_net_paid_pmpm_cost_3to6m_b4',\n",
    " 'rx_maint_pmpm_cost_t_12-9-6m_b4',\n",
    " 'auth_3mth_post_acute_rsk',\n",
    " 'rev_cms_ansth_pmpm_ct',\n",
    " 'cons_cwht']\n",
    "\n",
    "target = \"covid_vaccination\"\n",
    "\n",
    "student_id = 2000728661\n",
    "\n",
    "id = \"ID\"\n",
    "reg_scalar = MinMaxScaler()\n",
    "reg_f, cat_f = fdf = SelectKBest(f_regression, k=50), SelectKBest(f_classif, k=70)\n",
    "def scale_df(dataframe, train=True):\n",
    "    \n",
    "    if train: dataframe[reg_cols] = reg_scalar.fit_transform(dataframe[reg_cols])\n",
    "    else: dataframe[reg_cols] = reg_scalar.transform(dataframe[reg_cols])\n",
    "    return dataframe\n",
    "\n",
    "def get_reduced_features(dataframe, train=True):\n",
    "    rcat, rreg = [], []\n",
    "    if train: \n",
    "        rcat = cat_f.fit(dataframe[cat_cols], dataframe[target]).get_support(indices=True)\n",
    "        rreg = reg_f.fit(dataframe[reg_cols], dataframe[target]).get_support(indices=True)\n",
    "    else:\n",
    "        rcat = cat_f.get_support(indices=True)\n",
    "        rreg = reg_f.get_support(indices=True)\n",
    "    rcat = [i for idx, i in enumerate(dataframe[cat_cols].columns) if idx in rcat]\n",
    "    rreg = [i for idx, i in enumerate(dataframe[reg_cols].columns) if idx in rreg]\n",
    "    return rcat, rreg\n",
    "\n",
    "df = pd.read_csv('dataset/transformed_dataset.csv')\n",
    "\n",
    "# cat_cols, reg_cols = get_reduced_features(df)\n",
    "\n",
    "\n",
    "df = df[cat_cols + reg_cols + [target]]\n",
    "df = scale_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee604de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_T, y_T = df[reg_cols + cat_cols], df[target]\n",
    "# X_T, X_t, y_T, y_t = train_test_split(X, y, test_size=.2, random_state=student_id, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f79420",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth':[9, 10, 12],\n",
    "    'learning_rate': [.1],\n",
    "    'n_estimators': [200, 250]\n",
    "}\n",
    "\n",
    "# m = CatBoostClassifier(random_state = student_id, task_type=\"GPU\", devices='0:1', \n",
    "#                            eval_metric='AUC', thread_count=1, \n",
    "#                            cat_features=cat_cols, metric_period=40,\n",
    "#                            od_type='Iter', loss_function=\"Logloss\", \n",
    "#                        depth=12, learning_rate=learning_rate[1],n_estimators=n_estimators[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5c30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.fit(X_T,y=y_T,eval_set=(X_t, y_t),verbose=True,plot=True, use_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5c0562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, output_process=False):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y, params={'verbose': -1}, free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n",
    "        params = {'application':'binary', 'metric':'auc'}\n",
    "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['max_bin'] = int(round(max_depth))\n",
    "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        params['verbose']: -1\n",
    "        \n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, \n",
    "                           verbose_eval =200, metrics=['auc'], categorical_feature=cat_cols)\n",
    "        return max(cv_result['auc-mean'])\n",
    "     \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n",
    "                                            'num_leaves': (24, 80),\n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'bagging_fraction': (0.8, 1),\n",
    "                                            'max_depth': (5, 30),\n",
    "                                            'max_bin':(20,90),\n",
    "                                            'min_data_in_leaf': (20, 80),\n",
    "                                            'min_sum_hessian_in_leaf':(0,100),\n",
    "                                           'subsample': (0.01, 1.0)\n",
    "                                           }, \n",
    "                                 random_state=student_id)\n",
    "\n",
    "    \n",
    "    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n",
    "    \n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    model_auc=[]\n",
    "    for model in range(len( lgbBO.res)):\n",
    "        model_auc.append(lgbBO.res[model]['target'])\n",
    "    \n",
    "    # return best parameters\n",
    "    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97cfa0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9394553504959928, subsample=0.15203912244729975 will be ignored. Current value: bagging_fraction=0.9394553504959928\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3128\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9394553504959928, subsample=0.15203912244729975 will be ignored. Current value: bagging_fraction=0.9394553504959928\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3128\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9394553504959928, subsample=0.15203912244729975 will be ignored. Current value: bagging_fraction=0.9394553504959928\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.277019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3128\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9394553504959928, subsample=0.15203912244729975 will be ignored. Current value: bagging_fraction=0.9394553504959928\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6554  \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 0.6074  \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 65.42   \u001b[0m | \u001b[0m 11.41   \u001b[0m | \u001b[0m 25.52   \u001b[0m | \u001b[0m 82.24   \u001b[0m | \u001b[0m 58.57   \u001b[0m | \u001b[0m 0.152   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9656700131903726, subsample=0.027180498894965627 will be ignored. Current value: bagging_fraction=0.9656700131903726\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5235\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 273\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9656700131903726, subsample=0.027180498894965627 will be ignored. Current value: bagging_fraction=0.9656700131903726\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5235\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 273\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9656700131903726, subsample=0.027180498894965627 will be ignored. Current value: bagging_fraction=0.9656700131903726\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5235\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 273\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9656700131903726, subsample=0.027180498894965627 will be ignored. Current value: bagging_fraction=0.9656700131903726\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.6579  \u001b[0m | \u001b[95m 0.9657  \u001b[0m | \u001b[95m 0.1381  \u001b[0m | \u001b[95m 0.9864  \u001b[0m | \u001b[95m 39.29   \u001b[0m | \u001b[95m 26.4    \u001b[0m | \u001b[95m 67.65   \u001b[0m | \u001b[95m 28.85   \u001b[0m | \u001b[95m 40.19   \u001b[0m | \u001b[95m 0.02718 \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8528022715071093, subsample=0.07583011969122037 will be ignored. Current value: bagging_fraction=0.8528022715071093\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4139\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 281\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8528022715071093, subsample=0.07583011969122037 will be ignored. Current value: bagging_fraction=0.8528022715071093\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4139\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 281\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8528022715071093, subsample=0.07583011969122037 will be ignored. Current value: bagging_fraction=0.8528022715071093\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4139\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 281\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8528022715071093, subsample=0.07583011969122037 will be ignored. Current value: bagging_fraction=0.8528022715071093\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.6742  \u001b[0m | \u001b[95m 0.8528  \u001b[0m | \u001b[95m 0.2803  \u001b[0m | \u001b[95m 0.2268  \u001b[0m | \u001b[95m 24.09   \u001b[0m | \u001b[95m 18.47   \u001b[0m | \u001b[95m 29.98   \u001b[0m | \u001b[95m 17.46   \u001b[0m | \u001b[95m 49.41   \u001b[0m | \u001b[95m 0.07583 \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8209030603869213, subsample=0.25412142942977417 will be ignored. Current value: bagging_fraction=0.8209030603869213\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.283935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 275\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8209030603869213, subsample=0.25412142942977417 will be ignored. Current value: bagging_fraction=0.8209030603869213\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 275\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8209030603869213, subsample=0.25412142942977417 will be ignored. Current value: bagging_fraction=0.8209030603869213\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5380\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 275\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8209030603869213, subsample=0.25412142942977417 will be ignored. Current value: bagging_fraction=0.8209030603869213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.6571  \u001b[0m | \u001b[0m 0.8209  \u001b[0m | \u001b[0m 0.8487  \u001b[0m | \u001b[0m 0.8517  \u001b[0m | \u001b[0m 71.76   \u001b[0m | \u001b[0m 27.36   \u001b[0m | \u001b[0m 57.34   \u001b[0m | \u001b[0m 46.06   \u001b[0m | \u001b[0m 53.96   \u001b[0m | \u001b[0m 0.2541  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8294528638666859, subsample=0.5610635424687966 will be ignored. Current value: bagging_fraction=0.8294528638666859\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 277\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8294528638666859, subsample=0.5610635424687966 will be ignored. Current value: bagging_fraction=0.8294528638666859\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 277\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8294528638666859, subsample=0.5610635424687966 will be ignored. Current value: bagging_fraction=0.8294528638666859\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 277\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8294528638666859, subsample=0.5610635424687966 will be ignored. Current value: bagging_fraction=0.8294528638666859\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6621  \u001b[0m | \u001b[0m 0.8295  \u001b[0m | \u001b[0m 0.1512  \u001b[0m | \u001b[0m 0.5515  \u001b[0m | \u001b[0m 89.18   \u001b[0m | \u001b[0m 11.44   \u001b[0m | \u001b[0m 46.65   \u001b[0m | \u001b[0m 5.227   \u001b[0m | \u001b[0m 58.41   \u001b[0m | \u001b[0m 0.5611  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.802339045570122, subsample=0.40113772954220245 will be ignored. Current value: bagging_fraction=0.802339045570122\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4557\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 281\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.802339045570122, subsample=0.40113772954220245 will be ignored. Current value: bagging_fraction=0.802339045570122\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4557\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 281\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.802339045570122, subsample=0.40113772954220245 will be ignored. Current value: bagging_fraction=0.802339045570122\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4557\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 281\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.802339045570122, subsample=0.40113772954220245 will be ignored. Current value: bagging_fraction=0.802339045570122\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6701  \u001b[0m | \u001b[0m 0.8023  \u001b[0m | \u001b[0m 0.2302  \u001b[0m | \u001b[0m 0.4917  \u001b[0m | \u001b[0m 26.95   \u001b[0m | \u001b[0m 20.59   \u001b[0m | \u001b[0m 31.81   \u001b[0m | \u001b[0m 15.98   \u001b[0m | \u001b[0m 47.27   \u001b[0m | \u001b[0m 0.4011  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9997948523537895, subsample=0.01 will be ignored. Current value: bagging_fraction=0.9997948523537895\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3132\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9997948523537895, subsample=0.01 will be ignored. Current value: bagging_fraction=0.9997948523537895\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3132\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9997948523537895, subsample=0.01 will be ignored. Current value: bagging_fraction=0.9997948523537895\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3132\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9997948523537895, subsample=0.01 will be ignored. Current value: bagging_fraction=0.9997948523537895\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.667   \u001b[0m | \u001b[0m 0.9998  \u001b[0m | \u001b[0m 0.5408  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 24.44   \u001b[0m | \u001b[0m 11.04   \u001b[0m | \u001b[0m 23.57   \u001b[0m | \u001b[0m 17.64   \u001b[0m | \u001b[0m 52.06   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8065797634216274, subsample=0.904031312786574 will be ignored. Current value: bagging_fraction=0.8065797634216274\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4137\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 280\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8065797634216274, subsample=0.904031312786574 will be ignored. Current value: bagging_fraction=0.8065797634216274\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4137\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 280\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8065797634216274, subsample=0.904031312786574 will be ignored. Current value: bagging_fraction=0.8065797634216274\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4137\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 280\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8065797634216274, subsample=0.904031312786574 will be ignored. Current value: bagging_fraction=0.8065797634216274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6616  \u001b[0m | \u001b[0m 0.8066  \u001b[0m | \u001b[0m 0.2714  \u001b[0m | \u001b[0m 0.7571  \u001b[0m | \u001b[0m 23.55   \u001b[0m | \u001b[0m 17.72   \u001b[0m | \u001b[0m 35.98   \u001b[0m | \u001b[0m 18.69   \u001b[0m | \u001b[0m 56.66   \u001b[0m | \u001b[0m 0.904   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9483560080826936, subsample=0.6967677180575995 will be ignored. Current value: bagging_fraction=0.9483560080826936\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3295\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9483560080826936, subsample=0.6967677180575995 will be ignored. Current value: bagging_fraction=0.9483560080826936\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3295\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9483560080826936, subsample=0.6967677180575995 will be ignored. Current value: bagging_fraction=0.9483560080826936\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.298620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3295\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9483560080826936, subsample=0.6967677180575995 will be ignored. Current value: bagging_fraction=0.9483560080826936\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6733  \u001b[0m | \u001b[0m 0.9484  \u001b[0m | \u001b[0m 0.3448  \u001b[0m | \u001b[0m 0.2103  \u001b[0m | \u001b[0m 21.91   \u001b[0m | \u001b[0m 12.34   \u001b[0m | \u001b[0m 27.14   \u001b[0m | \u001b[0m 21.04   \u001b[0m | \u001b[0m 47.31   \u001b[0m | \u001b[0m 0.6968  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4705\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4705\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4705\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.6755  \u001b[0m | \u001b[95m 0.8855  \u001b[0m | \u001b[95m 0.4436  \u001b[0m | \u001b[95m 0.1133  \u001b[0m | \u001b[95m 22.78   \u001b[0m | \u001b[95m 22.21   \u001b[0m | \u001b[95m 25.64   \u001b[0m | \u001b[95m 15.19   \u001b[0m | \u001b[95m 49.33   \u001b[0m | \u001b[95m 0.897   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8250640682977964, subsample=0.6902843838962323 will be ignored. Current value: bagging_fraction=0.8250640682977964\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4571\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8250640682977964, subsample=0.6902843838962323 will be ignored. Current value: bagging_fraction=0.8250640682977964\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4571\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8250640682977964, subsample=0.6902843838962323 will be ignored. Current value: bagging_fraction=0.8250640682977964\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4571\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8250640682977964, subsample=0.6902843838962323 will be ignored. Current value: bagging_fraction=0.8250640682977964\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6732  \u001b[0m | \u001b[0m 0.8251  \u001b[0m | \u001b[0m 0.2612  \u001b[0m | \u001b[0m 0.2989  \u001b[0m | \u001b[0m 22.69   \u001b[0m | \u001b[0m 20.89   \u001b[0m | \u001b[0m 22.55   \u001b[0m | \u001b[0m 16.56   \u001b[0m | \u001b[0m 39.55   \u001b[0m | \u001b[0m 0.6903  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8326807205193224, subsample=0.474561980202887 will be ignored. Current value: bagging_fraction=0.8326807205193224\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5406\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8326807205193224, subsample=0.474561980202887 will be ignored. Current value: bagging_fraction=0.8326807205193224\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5406\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8326807205193224, subsample=0.474561980202887 will be ignored. Current value: bagging_fraction=0.8326807205193224\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5406\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8326807205193224, subsample=0.474561980202887 will be ignored. Current value: bagging_fraction=0.8326807205193224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6649  \u001b[0m | \u001b[0m 0.8327  \u001b[0m | \u001b[0m 0.5664  \u001b[0m | \u001b[0m 0.5681  \u001b[0m | \u001b[0m 28.66   \u001b[0m | \u001b[0m 27.46   \u001b[0m | \u001b[0m 21.45   \u001b[0m | \u001b[0m 29.49   \u001b[0m | \u001b[0m 49.16   \u001b[0m | \u001b[0m 0.4746  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8788720530496792, subsample=0.6509069844649087 will be ignored. Current value: bagging_fraction=0.8788720530496792\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3285\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 281\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8788720530496792, subsample=0.6509069844649087 will be ignored. Current value: bagging_fraction=0.8788720530496792\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3285\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 281\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8788720530496792, subsample=0.6509069844649087 will be ignored. Current value: bagging_fraction=0.8788720530496792\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3285\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 281\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8788720530496792, subsample=0.6509069844649087 will be ignored. Current value: bagging_fraction=0.8788720530496792\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6649  \u001b[0m | \u001b[0m 0.8789  \u001b[0m | \u001b[0m 0.5922  \u001b[0m | \u001b[0m 0.6298  \u001b[0m | \u001b[0m 22.88   \u001b[0m | \u001b[0m 12.0    \u001b[0m | \u001b[0m 32.68   \u001b[0m | \u001b[0m 24.2    \u001b[0m | \u001b[0m 33.84   \u001b[0m | \u001b[0m 0.6509  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382656842820795, subsample=0.47905809108442016 will be ignored. Current value: bagging_fraction=0.8382656842820795\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3581\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382656842820795, subsample=0.47905809108442016 will be ignored. Current value: bagging_fraction=0.8382656842820795\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3581\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382656842820795, subsample=0.47905809108442016 will be ignored. Current value: bagging_fraction=0.8382656842820795\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3581\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8382656842820795, subsample=0.47905809108442016 will be ignored. Current value: bagging_fraction=0.8382656842820795\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6754  \u001b[0m | \u001b[0m 0.8383  \u001b[0m | \u001b[0m 0.8218  \u001b[0m | \u001b[0m 0.07717 \u001b[0m | \u001b[0m 24.12   \u001b[0m | \u001b[0m 14.41   \u001b[0m | \u001b[0m 26.45   \u001b[0m | \u001b[0m 2.493   \u001b[0m | \u001b[0m 43.4    \u001b[0m | \u001b[0m 0.4791  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8434497289795996, subsample=0.9709862210827052 will be ignored. Current value: bagging_fraction=0.8434497289795996\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4433\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8434497289795996, subsample=0.9709862210827052 will be ignored. Current value: bagging_fraction=0.8434497289795996\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4433\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8434497289795996, subsample=0.9709862210827052 will be ignored. Current value: bagging_fraction=0.8434497289795996\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4433\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 288\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8434497289795996, subsample=0.9709862210827052 will be ignored. Current value: bagging_fraction=0.8434497289795996\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6688  \u001b[0m | \u001b[0m 0.8434  \u001b[0m | \u001b[0m 0.853   \u001b[0m | \u001b[0m 0.4415  \u001b[0m | \u001b[0m 22.91   \u001b[0m | \u001b[0m 20.19   \u001b[0m | \u001b[0m 22.45   \u001b[0m | \u001b[0m 0.8912  \u001b[0m | \u001b[0m 33.72   \u001b[0m | \u001b[0m 0.971   \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "opt_params = bayes_parameter_opt_lgb(X_T, y_T, init_round=5, opt_round=10, n_folds=3, random_seed=student_id,n_estimators=10000)\n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='binary'\n",
    "opt_params[1]['metric']='auc'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['boost_from_average']=False\n",
    "opt_params=opt_params[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "381141cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8855002052522746,\n",
       " 'feature_fraction': 0.4435584169139978,\n",
       " 'learning_rate': 0.11326924831768609,\n",
       " 'max_bin': 23,\n",
       " 'max_depth': 22,\n",
       " 'min_data_in_leaf': 26,\n",
       " 'min_sum_hessian_in_leaf': 15.18694006998691,\n",
       " 'num_leaves': 49,\n",
       " 'subsample': 0.8970360999639048,\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'is_unbalance': True,\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "550fcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# m = CatBoostClassifier(random_state = student_id, task_type=\"GPU\", devices='0:1', \n",
    "#                            eval_metric='AUC', thread_count=1, \n",
    "#                            cat_features=cat_cols, metric_period=40,\n",
    "#                            od_type='Iter', loss_function=\"Logloss\",\n",
    "#                        depth=9, learning_rate=.1,n_estimators=200)\n",
    "\n",
    "# # best_params = m.grid_search(params, X,y=y, plot=True, stratified=True, cv=4, verbose=True)\n",
    "# m.fit(X, y=y, plot=True, )\n",
    "\n",
    "# m.save_model('models/catboost.cbm',\n",
    "#            format=\"cbm\",\n",
    "#            export_parameters=None,\n",
    "#            pool=None)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f49fb0d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adec86b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 152507, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4851\n",
      "[LightGBM] [Info] Number of data points in the train set: 877357, number of used features: 287\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's auc: 0.719469\tvalid_1's auc: 0.677969\n",
      "Fold 1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 152507, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4837\n",
      "[LightGBM] [Info] Number of data points in the train set: 877357, number of used features: 282\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[109]\ttraining's auc: 0.727599\tvalid_1's auc: 0.674185\n",
      "Fold 2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[106]\ttraining's auc: 0.725037\tvalid_1's auc: 0.676482\n",
      "Fold 3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4840\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 283\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[87]\ttraining's auc: 0.717776\tvalid_1's auc: 0.677943\n",
      "Fold 4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4848\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[94]\ttraining's auc: 0.721004\tvalid_1's auc: 0.67764\n",
      "Fold 5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4851\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 289\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttraining's auc: 0.727191\tvalid_1's auc: 0.677312\n",
      "Fold 6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4846\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 286\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's auc: 0.722799\tvalid_1's auc: 0.678092\n",
      "Fold 7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4849\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 287\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[115]\ttraining's auc: 0.729188\tvalid_1's auc: 0.679894\n",
      "Fold 8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4844\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 285\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[81]\ttraining's auc: 0.71461\tvalid_1's auc: 0.678083\n",
      "Fold 9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "[LightGBM] [Info] Number of positive: 152507, number of negative: 724851\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4851\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 287\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855002052522746, subsample=0.8970360999639048 will be ignored. Current value: bagging_fraction=0.8855002052522746\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\ttraining's auc: 0.737661\tvalid_1's auc: 0.678088\n"
     ]
    }
   ],
   "source": [
    "tdf = pd.read_csv('dataset/transformed_dataset_holdout.csv')\n",
    "tdf = tdf[reg_cols + cat_cols + [id]]\n",
    "tdf = scale_df(tdf, False)\n",
    "\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=31416)\n",
    "oof = np.zeros(len(X_T))\n",
    "predictions = np.zeros(len(tdf))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "features = reg_cols + cat_cols\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_T.values, y_T.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(X_T.iloc[trn_idx][features], label=y_T.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(X_T.iloc[val_idx][features], label=y_T.iloc[val_idx])\n",
    "\n",
    "    num_round = 15000\n",
    "    clf = lgb.train(opt_params, trn_data, num_round, valid_sets = [trn_data, val_data], \n",
    "                    verbose_eval=500, early_stopping_rounds = 250, categorical_feature=cat_cols)\n",
    "    oof[val_idx] = clf.predict(X_T.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(tdf[features], num_iteration=clf.best_iteration) / folds.n_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d950b33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atlas_pct_diabetes_adults13</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atlas_pct_wic15</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_physician_office_net_paid_pmpm_cost_9to1...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atlas_pct_laccess_hisp15</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atlas_pct_fmrkt_frveg16</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>bh_ip_snf_net_paid_pmpm_cost_3to6m_b4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>rx_maint_pmpm_cost_t_12-9-6m_b4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>auth_3mth_post_acute_rsk</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>rev_cms_ansth_pmpm_ct</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>cons_cwht</td>\n",
       "      <td>779</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Feature  importance  fold\n",
       "0                          atlas_pct_diabetes_adults13           4    10\n",
       "1                                      atlas_pct_wic15           4    10\n",
       "2    total_physician_office_net_paid_pmpm_cost_9to1...           0    10\n",
       "3                             atlas_pct_laccess_hisp15           1    10\n",
       "4                              atlas_pct_fmrkt_frveg16           2    10\n",
       "..                                                 ...         ...   ...\n",
       "325              bh_ip_snf_net_paid_pmpm_cost_3to6m_b4           0    10\n",
       "326                    rx_maint_pmpm_cost_t_12-9-6m_b4          10    10\n",
       "327                           auth_3mth_post_acute_rsk           0    10\n",
       "328                              rev_cms_ansth_pmpm_ct           3    10\n",
       "329                                          cons_cwht         779    10\n",
       "\n",
       "[330 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f2cff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((974842,),)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e390401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.67755 \n"
     ]
    }
   ],
   "source": [
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(y_T, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a805c9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.41475253, 0.19525717, 0.51993198, ..., 0.63572648, 0.54156594,\n",
       "        0.7428567 ]),\n",
       " numpy.ndarray)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ebbcf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tdf[target] = predictions > .5\n",
    "tdf['SCORE'] = 1. - predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d95f8cde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tdf['RANK'] = tdf['SCORE'].rank(ascending=False, method='first').astype(np.int64)\n",
    "tdf['SCORE'] = tdf['SCORE'].round(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a13b0b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    805389\n",
       "1    169453\n",
       "Name: covid_vaccination, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_T.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4820b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_csv('dataset/2021CaseCompetition_Ashutosh_Tiwari_20211006-2.csv', index=False, columns=[id, 'SCORE', 'RANK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2451105c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525158, 334)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "915ff73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    286984\n",
       "True     238174\n",
       "Name: covid_vaccination, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5322bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atlas_pct_diabetes_adults13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atlas_pct_wic15</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_physician_office_net_paid_pmpm_cost_9to1...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atlas_pct_laccess_hisp15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atlas_pct_fmrkt_frveg16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>bh_ip_snf_net_paid_pmpm_cost_3to6m_b4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>rx_maint_pmpm_cost_t_12-9-6m_b4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>auth_3mth_post_acute_rsk</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>rev_cms_ansth_pmpm_ct</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>cons_cwht</td>\n",
       "      <td>779</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3300 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Feature  importance  fold\n",
       "0                          atlas_pct_diabetes_adults13           1     1\n",
       "1                                      atlas_pct_wic15           7     1\n",
       "2    total_physician_office_net_paid_pmpm_cost_9to1...           0     1\n",
       "3                             atlas_pct_laccess_hisp15           0     1\n",
       "4                              atlas_pct_fmrkt_frveg16           2     1\n",
       "..                                                 ...         ...   ...\n",
       "325              bh_ip_snf_net_paid_pmpm_cost_3to6m_b4           0    10\n",
       "326                    rx_maint_pmpm_cost_t_12-9-6m_b4          10    10\n",
       "327                           auth_3mth_post_acute_rsk           0    10\n",
       "328                              rev_cms_ansth_pmpm_ct           3    10\n",
       "329                                          cons_cwht         779    10\n",
       "\n",
       "[3300 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc02df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

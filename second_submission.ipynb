{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "942aabd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rx_maint_net_paid_pmpm_cost_t_9-6-3m_b4</th>\n",
       "      <th>atlas_hiamenity</th>\n",
       "      <th>cons_ltmedicr</th>\n",
       "      <th>cons_n2pwh</th>\n",
       "      <th>rx_gpi2_56_dist_gpi6_pmpm_ct_3to6m_b4</th>\n",
       "      <th>atlas_low_employment_2015_update</th>\n",
       "      <th>lab_albumin_loinc_pmpm_ct</th>\n",
       "      <th>cons_rxadhs</th>\n",
       "      <th>cons_mobplus</th>\n",
       "      <th>atlas_foodinsec_child_03_11</th>\n",
       "      <th>...</th>\n",
       "      <th>atlas_povertyallagespct</th>\n",
       "      <th>rx_nonbh_mbr_resp_pmpm_cost</th>\n",
       "      <th>lab_dist_loinc_pmpm_ct</th>\n",
       "      <th>rx_generic_mbr_resp_pmpm_cost</th>\n",
       "      <th>credit_bal_nonmtgcredit_60dpd</th>\n",
       "      <th>rx_overall_mbr_resp_pmpm_cost</th>\n",
       "      <th>atlas_percapitainc</th>\n",
       "      <th>phy_em_px_pct</th>\n",
       "      <th>rx_generic_mbr_resp_pmpm_cost_0to3m_b4</th>\n",
       "      <th>covid_vaccination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252212</td>\n",
       "      <td>0.008587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025524</td>\n",
       "      <td>0.159828</td>\n",
       "      <td>0.008597</td>\n",
       "      <td>0.351813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294248</td>\n",
       "      <td>0.028989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067735</td>\n",
       "      <td>0.197643</td>\n",
       "      <td>0.016252</td>\n",
       "      <td>0.382757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227876</td>\n",
       "      <td>0.016262</td>\n",
       "      <td>0.040441</td>\n",
       "      <td>0.021612</td>\n",
       "      <td>0.305488</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>0.363665</td>\n",
       "      <td>0.130694</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360619</td>\n",
       "      <td>0.004277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012725</td>\n",
       "      <td>0.269741</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>0.300465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.536348</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rx_maint_net_paid_pmpm_cost_t_9-6-3m_b4  atlas_hiamenity  cons_ltmedicr  \\\n",
       "0                                        1                0              4   \n",
       "1                                        8                0              3   \n",
       "2                                        0                0              0   \n",
       "3                                        0                0              8   \n",
       "4                                        4                1              4   \n",
       "\n",
       "   cons_n2pwh  rx_gpi2_56_dist_gpi6_pmpm_ct_3to6m_b4  \\\n",
       "0          95                                      1   \n",
       "1          93                                      2   \n",
       "2          26                                      1   \n",
       "3          38                                      1   \n",
       "4          88                                      1   \n",
       "\n",
       "   atlas_low_employment_2015_update  lab_albumin_loinc_pmpm_ct  cons_rxadhs  \\\n",
       "0                                 0                          1            5   \n",
       "1                                 0                          1            0   \n",
       "2                                 0                          1            9   \n",
       "3                                 0                          1            4   \n",
       "4                                 0                          1            5   \n",
       "\n",
       "   cons_mobplus  atlas_foodinsec_child_03_11  ...  atlas_povertyallagespct  \\\n",
       "0             0                           12  ...                 0.252212   \n",
       "1             2                            5  ...                 0.294248   \n",
       "2             2                           21  ...                 0.227876   \n",
       "3             0                           27  ...                 0.360619   \n",
       "4             2                           28  ...                 0.258850   \n",
       "\n",
       "   rx_nonbh_mbr_resp_pmpm_cost  lab_dist_loinc_pmpm_ct  \\\n",
       "0                     0.008587                0.000000   \n",
       "1                     0.028989                0.000000   \n",
       "2                     0.016262                0.040441   \n",
       "3                     0.004277                0.000000   \n",
       "4                     0.000000                0.000000   \n",
       "\n",
       "   rx_generic_mbr_resp_pmpm_cost  credit_bal_nonmtgcredit_60dpd  \\\n",
       "0                       0.025524                       0.159828   \n",
       "1                       0.067735                       0.197643   \n",
       "2                       0.021612                       0.305488   \n",
       "3                       0.012725                       0.269741   \n",
       "4                       0.000000                       0.090292   \n",
       "\n",
       "   rx_overall_mbr_resp_pmpm_cost  atlas_percapitainc  phy_em_px_pct  \\\n",
       "0                       0.008597            0.351813       0.000000   \n",
       "1                       0.016252            0.382757       0.000000   \n",
       "2                       0.014049            0.363665       0.130694   \n",
       "3                       0.003586            0.300465       0.000000   \n",
       "4                       0.000000            0.536348       0.500000   \n",
       "\n",
       "   rx_generic_mbr_resp_pmpm_cost_0to3m_b4  covid_vaccination  \n",
       "0                                0.018443                  0  \n",
       "1                                0.059102                  0  \n",
       "2                                0.010014                  0  \n",
       "3                                0.012345                  0  \n",
       "4                                0.000000                  0  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
    "import gc\n",
    "gc.enable()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "file = \"dataset/transformed_dataset.csv\"\n",
    "reg_cols = ['atlas_pct_diabetes_adults13',\n",
    " 'atlas_pct_wic15',\n",
    " 'total_physician_office_net_paid_pmpm_cost_9to12m_b4',\n",
    " 'atlas_pct_laccess_hisp15',\n",
    " 'atlas_pct_fmrkt_frveg16',\n",
    " 'credit_hh_nonmtgcredit_60dpd',\n",
    " 'atlas_dirsales_farms12',\n",
    " 'rx_nonmaint_pmpm_ct',\n",
    " 'zip_cd',\n",
    " 'atlas_pct_laccess_white15',\n",
    " 'credit_hh_bankcard_severederog',\n",
    " 'atlas_pct_fmrkt_credit16',\n",
    " 'credit_bal_autofinance_new',\n",
    " 'rej_days_since_last_clm',\n",
    " 'rx_generic_pmpm_ct_0to3m_b4',\n",
    " 'rwjf_social_associate_rate',\n",
    " 'med_physician_office_ds_clm_6to9m_b4',\n",
    " 'atlas_totalocchu',\n",
    " 'atlas_veg_acrespth12',\n",
    " 'atlas_pct_loclsale12',\n",
    " 'atlas_pct_fmrkt_anmlprod16',\n",
    " 'atlas_freshveg_farms12',\n",
    " 'rwjf_resident_seg_black_inx',\n",
    " 'atlas_pct_loclfarm12',\n",
    " 'total_outpatient_mbr_resp_pmpm_cost_6to9m_b4',\n",
    " 'atlas_berry_acrespth12',\n",
    " 'rx_maint_pmpm_ct_9to12m_b4',\n",
    " 'rx_tier_2_pmpm_ct',\n",
    " 'atlas_agritrsm_rct12',\n",
    " 'atlas_pct_laccess_snap15',\n",
    " 'atlas_deep_pov_all',\n",
    " 'ccsp_227_pct',\n",
    " 'bh_outpatient_net_paid_pmpm_cost',\n",
    " 'atlas_veg_farms12',\n",
    " 'rx_hum_16_pmpm_ct',\n",
    " 'cms_risk_adjustment_factor_a_amt',\n",
    " 'atlas_recfac14',\n",
    " 'total_physician_office_copay_pmpm_cost',\n",
    " 'atlas_pc_fsrsales12',\n",
    " 'atlas_pct_fmrkt_baked16',\n",
    " 'atlas_net_international_migration_rate',\n",
    " 'rx_maint_mbr_resp_pmpm_cost_6to9m_b4',\n",
    " 'rx_generic_pmpm_cost_6to9m_b4',\n",
    " 'rx_gpi2_49_pmpm_cost_0to3m_b4',\n",
    " 'atlas_pct_sbp15',\n",
    " 'atlas_pct_laccess_child15',\n",
    " 'met_obe_diag_pct',\n",
    " 'atlas_orchard_acrespth12',\n",
    " 'atlas_pct_laccess_hhnv15',\n",
    " 'cnt_cp_webstatement_pmpm_ct',\n",
    " 'atlas_pct_laccess_lowi15',\n",
    " 'rx_gpi2_02_pmpm_cost',\n",
    " 'cms_partd_ra_factor_amt',\n",
    " 'atlas_pct_free_lunch14',\n",
    " 'rx_tier_2_pmpm_ct_3to6m_b4',\n",
    " 'cons_chva',\n",
    " 'atlas_pct_fmrkt_wiccash16',\n",
    " 'rx_overall_net_paid_pmpm_cost_6to9m_b4',\n",
    " 'total_med_allowed_pmpm_cost_9to12m_b4',\n",
    " 'bh_physician_office_copay_pmpm_cost_6to9m_b4',\n",
    " 'atlas_pct_snap16',\n",
    " 'atlas_ghveg_sqftpth12',\n",
    " 'atlas_pc_dirsales12',\n",
    " 'atlas_pct_reduced_lunch14',\n",
    " 'ccsp_236_pct',\n",
    " 'atlas_deep_pov_children',\n",
    " 'atlas_pct_sfsp15',\n",
    " 'rwjf_air_pollute_density',\n",
    " 'rx_generic_pmpm_cost',\n",
    " 'cms_tot_partd_payment_amt',\n",
    " 'cons_nwperadult',\n",
    " 'rx_days_since_last_script',\n",
    " 'atlas_pct_laccess_nhasian15',\n",
    " 'rx_nonbh_mbr_resp_pmpm_cost_6to9m_b4',\n",
    " 'rx_days_since_last_script_6to9m_b4',\n",
    " 'atlas_pct_obese_adults13',\n",
    " 'credit_bal_consumerfinance',\n",
    " 'atlas_pct_fmrkt_wic16',\n",
    " 'atlas_orchard_farms12',\n",
    " 'atlas_berry_farms12',\n",
    " 'atlas_pct_laccess_multir15',\n",
    " 'rx_bh_mbr_resp_pmpm_cost_9to12m_b4',\n",
    " 'atlas_pc_wic_redemp12',\n",
    " 'rwjf_mv_deaths_rate',\n",
    " 'atlas_povertyunder18pct',\n",
    " 'rx_gpi2_72_pmpm_cost_6to9m_b4',\n",
    " 'atlas_pct_fmrkt_snap16',\n",
    " 'atlas_medhhinc',\n",
    " 'rx_nonbh_net_paid_pmpm_cost',\n",
    " 'credit_bal_bankcard_severederog',\n",
    " 'bh_ip_snf_net_paid_pmpm_cost',\n",
    " 'atlas_pc_snapben15',\n",
    " 'rx_nonbh_pmpm_ct_0to3m_b4',\n",
    " 'rx_overall_mbr_resp_pmpm_cost_0to3m_b4',\n",
    " 'auth_3mth_post_acute_mean_los',\n",
    " 'rx_branded_mbr_resp_pmpm_cost',\n",
    " 'rx_tier_1_pmpm_ct_0to3m_b4',\n",
    " 'bh_ncdm_pct',\n",
    " 'atlas_naturalchangerate1016',\n",
    " 'rx_mail_mbr_resp_pmpm_cost_0to3m_b4',\n",
    " 'credit_bal_autobank',\n",
    " 'rx_nonotc_dist_gpi6_pmpm_ct',\n",
    " 'cons_cgqs',\n",
    " 'rx_overall_gpi_pmpm_ct_0to3m_b4',\n",
    " 'credit_hh_bankcardcredit_60dpd',\n",
    " 'rx_gpi2_01_pmpm_cost_0to3m_b4',\n",
    " 'cci_dia_m_pmpm_ct',\n",
    " 'atlas_pct_nslp15',\n",
    " 'mcc_end_pct',\n",
    " 'atlas_pct_laccess_black15',\n",
    " 'credit_bal_mtgcredit_new',\n",
    " 'credit_hh_1stmtgcredit',\n",
    " 'cons_chmi',\n",
    " 'rwjf_income_inequ_ratio',\n",
    " 'atlas_pct_laccess_pop15',\n",
    " 'atlas_pc_ffrsales12',\n",
    " 'atlas_hh65plusalonepct',\n",
    " 'atlas_pct_fmrkt_sfmnp16',\n",
    " 'auth_3mth_acute_mean_los',\n",
    " 'rx_hum_28_pmpm_cost',\n",
    " 'atlas_pct_laccess_nhna15',\n",
    " 'atlas_povertyallagespct',\n",
    " 'rx_nonbh_mbr_resp_pmpm_cost',\n",
    " 'rx_nonmaint_mbr_resp_pmpm_cost_9to12m_b4',\n",
    " 'atlas_pct_fmrkt_otherfood16',\n",
    " 'lab_dist_loinc_pmpm_ct',\n",
    " 'rx_generic_mbr_resp_pmpm_cost',\n",
    " 'atlas_pct_laccess_seniors15',\n",
    " 'atlas_pct_cacfp15',\n",
    " 'total_outpatient_allowed_pmpm_cost_6to9m_b4',\n",
    " 'rx_nonmaint_mbr_resp_pmpm_cost',\n",
    " 'credit_bal_nonmtgcredit_60dpd',\n",
    " 'atlas_ownhomepct',\n",
    " 'rx_overall_mbr_resp_pmpm_cost',\n",
    " 'atlas_redemp_snaps16',\n",
    " 'atlas_netmigrationrate1016',\n",
    " 'atlas_percapitainc',\n",
    " 'phy_em_px_pct',\n",
    " 'rx_generic_mbr_resp_pmpm_cost_0to3m_b4']\n",
    "\n",
    "cat_cols = ['bh_ncdm_ind',\n",
    " 'auth_3mth_post_acute_inf',\n",
    " 'rx_maint_net_paid_pmpm_cost_t_9-6-3m_b4',\n",
    " 'ccsp_065_pmpm_ct',\n",
    " 'auth_3mth_acute_vco',\n",
    " 'rx_gpi2_72_pmpm_ct_6to9m_b4',\n",
    " 'auth_3mth_post_acute_men',\n",
    " 'rej_total_physician_office_visit_ct_pmpm_0to3m_b4',\n",
    " 'total_physician_office_net_paid_pmpm_cost_t_9-6-3m_b4',\n",
    " 'bh_ip_snf_net_paid_pmpm_cost_0to3m_b4',\n",
    " 'mcc_ano_pmpm_ct_t_9-6-3m_b4',\n",
    " 'atlas_type_2015_update',\n",
    " 'atlas_retirement_destination_2015_upda',\n",
    " 'auth_3mth_post_acute_sns',\n",
    " 'atlas_hiamenity',\n",
    " 'cons_ltmedicr',\n",
    " 'auth_3mth_acute_ccs_086',\n",
    " 'total_physician_office_mbr_resp_pmpm_cost_t_9-6-3m_b4',\n",
    " 'auth_3mth_acute_cir',\n",
    " 'atlas_csa12',\n",
    " 'total_med_net_paid_pmpm_cost_t_6-3-0m_b4',\n",
    " 'cons_n2pwh',\n",
    " 'auth_3mth_snf_post_hsp',\n",
    " 'auth_3mth_post_acute_inj',\n",
    " 'med_outpatient_mbr_resp_pmpm_cost_t_9-6-3m_b4',\n",
    " 'rx_gpi2_56_dist_gpi6_pmpm_ct_3to6m_b4',\n",
    " 'atlas_low_employment_2015_update',\n",
    " 'auth_3mth_acute_inf',\n",
    " 'lab_albumin_loinc_pmpm_ct',\n",
    " 'rx_gpi2_17_pmpm_cost_t_12-9-6m_b4',\n",
    " 'cons_rxadhs',\n",
    " 'cons_mobplus',\n",
    " 'atlas_foodinsec_child_03_11',\n",
    " 'lang_spoken_cd',\n",
    " 'bh_ip_snf_mbr_resp_pmpm_cost_9to12m_b4',\n",
    " 'auth_3mth_post_acute_gus',\n",
    " 'auth_3mth_acute_cad',\n",
    " 'rx_maint_pmpm_ct_t_6-3-0m_b4',\n",
    " 'auth_3mth_acute_ccs_044',\n",
    " 'cons_hxmioc',\n",
    " 'med_outpatient_visit_ct_pmpm_t_12-9-6m_b4',\n",
    " 'med_physician_office_allowed_pmpm_cost_t_9-6-3m_b4',\n",
    " 'auth_3mth_acute_res',\n",
    " 'auth_3mth_acute_chf',\n",
    " 'auth_3mth_acute_ccs_030',\n",
    " 'auth_3mth_dc_hospice',\n",
    " 'auth_3mth_acute_neo',\n",
    " 'atlas_type_2015_recreation_no',\n",
    " 'hum_region',\n",
    " 'atlas_ghveg_farms12',\n",
    " 'rx_maint_net_paid_pmpm_cost_t_12-9-6m_b4',\n",
    " 'auth_3mth_acute_ccs_048',\n",
    " 'rx_overall_gpi_pmpm_ct_t_6-3-0m_b4',\n",
    " 'rx_overall_gpi_pmpm_ct_t_12-9-6m_b4',\n",
    " 'rx_nonbh_pmpm_ct_t_9-6-3m_b4',\n",
    " 'mcc_chf_pmpm_ct_t_9-6-3m_b4',\n",
    " 'auth_3mth_post_acute_chf',\n",
    " 'auth_3mth_psychic',\n",
    " 'rx_nonotc_pmpm_cost_t_6-3-0m_b4',\n",
    " 'auth_3mth_acute_end',\n",
    " 'atlas_low_education_2015_update',\n",
    " 'src_div_id',\n",
    " 'auth_3mth_bh_acute',\n",
    " 'auth_3mth_acute_ccs_067',\n",
    " 'atlas_type_2015_mining_no',\n",
    " 'cons_n2pmr',\n",
    " 'rx_mail_net_paid_pmpm_cost_t_6-3-0m_b4',\n",
    " 'rej_med_er_net_paid_pmpm_cost_t_9-6-3m_b4',\n",
    " 'med_outpatient_deduct_pmpm_cost_t_9-6-3m_b4',\n",
    " 'rej_med_ip_snf_coins_pmpm_cost_t_9-6-3m_b4',\n",
    " 'rx_generic_dist_gpi6_pmpm_ct_t_9-6-3m_b4',\n",
    " 'auth_3mth_dc_home',\n",
    " 'auth_3mth_acute_bld',\n",
    " 'auth_3mth_acute_ner',\n",
    " 'oontwk_mbr_resp_pmpm_cost_t_6-3-0m_b4',\n",
    " 'rx_gpi2_90_dist_gpi6_pmpm_ct_9to12m_b4',\n",
    " 'atlas_foodhub16',\n",
    " 'rx_maint_pmpm_cost_t_6-3-0m_b4',\n",
    " 'auth_3mth_post_acute_ben',\n",
    " 'est_age',\n",
    " 'auth_3mth_post_acute_cer',\n",
    " 'auth_3mth_acute_ccs_153',\n",
    " 'auth_3mth_acute_dig',\n",
    " 'total_ip_maternity_net_paid_pmpm_cost_t_12-9-6m_b4',\n",
    " 'auth_3mth_post_acute_cad',\n",
    " 'rx_bh_pmpm_ct_0to3m_b4',\n",
    " 'rx_nonmail_dist_gpi6_pmpm_ct_t_9-6-3m_b4',\n",
    " 'atlas_persistentchildpoverty_1980_2011',\n",
    " 'atlas_slhouse12',\n",
    " 'atlas_population_loss_2015_update',\n",
    " 'auth_3mth_acute_ccs_094',\n",
    " 'auth_3mth_post_acute_ner',\n",
    " 'auth_3mth_acute_ccs_227',\n",
    " 'rx_overall_dist_gpi6_pmpm_ct_t_6-3-0m_b4',\n",
    " 'auth_3mth_acute_trm',\n",
    " 'auth_3mth_post_acute',\n",
    " 'auth_3mth_acute_dia',\n",
    " 'auth_3mth_acute_ccs_043',\n",
    " 'rx_overall_mbr_resp_pmpm_cost_t_6-3-0m_b4',\n",
    " 'cms_orig_reas_entitle_cd',\n",
    " 'auth_3mth_post_acute_end',\n",
    " 'auth_3mth_acute_can',\n",
    " 'auth_3mth_acute_ccs_172',\n",
    " 'auth_3mth_dc_home_health',\n",
    " 'atlas_hipov_1115',\n",
    " 'rx_phar_cat_cvs_pmpm_ct_t_9-6-3m_b4',\n",
    " 'rx_gpi2_62_pmpm_cost_t_9-6-3m_b4',\n",
    " 'cons_n2phi',\n",
    " 'auth_3mth_post_acute_hdz',\n",
    " 'auth_3mth_bh_acute_mean_los',\n",
    " 'auth_3mth_post_acute_dig',\n",
    " 'auth_3mth_transplant',\n",
    " 'rx_mail_mbr_resp_pmpm_cost_t_9-6-3m_b4',\n",
    " 'auth_3mth_acute_sns',\n",
    " 'auth_3mth_post_acute_vco',\n",
    " 'auth_3mth_home',\n",
    " 'rx_nonbh_net_paid_pmpm_cost_t_6-3-0m_b4',\n",
    " 'auth_3mth_post_acute_ckd',\n",
    " 'rx_gpi2_34_dist_gpi6_pmpm_ct',\n",
    " 'rx_gpi2_33_pmpm_ct_0to3m_b4',\n",
    " 'auth_3mth_dc_ltac',\n",
    " 'cons_estinv30_rc',\n",
    " 'rx_phar_cat_humana_pmpm_ct_t_9-6-3m_b4',\n",
    " 'auth_3mth_acute_men',\n",
    " 'auth_3mth_dc_snf',\n",
    " 'cons_hhcomp',\n",
    " 'bh_ip_snf_mbr_resp_pmpm_cost_6to9m_b4',\n",
    " 'auth_3mth_acute_inj',\n",
    " 'total_physician_office_visit_ct_pmpm_t_6-3-0m_b4',\n",
    " 'mabh_seg',\n",
    " 'auth_3mth_post_acute_res',\n",
    " 'auth_3mth_bh_acute_men',\n",
    " 'auth_3mth_acute_hdz',\n",
    " 'hedis_dia_hba1c_ge9',\n",
    " 'auth_3mth_post_acute_trm',\n",
    " 'auth_3mth_hospice',\n",
    " 'rx_gpi2_39_pmpm_cost_t_6-3-0m_b4',\n",
    " 'atlas_vlfoodsec_13_15',\n",
    " 'auth_3mth_dc_acute_rehab',\n",
    " 'rx_generic_pmpm_cost_t_6-3-0m_b4',\n",
    " 'auth_3mth_acute_ccs_154',\n",
    " 'cons_rxmaint',\n",
    " 'total_bh_copay_pmpm_cost_t_9-6-3m_b4',\n",
    " 'rx_nonmaint_dist_gpi6_pmpm_ct_t_12-9-6m_b4',\n",
    " 'rej_med_outpatient_visit_ct_pmpm_t_6-3-0m_b4',\n",
    " 'cons_rxadhm',\n",
    " 'auth_3mth_acute_mus',\n",
    " 'rx_nonbh_pmpm_cost_t_9-6-3m_b4',\n",
    " 'rx_days_since_last_script_0to3m_b4',\n",
    " 'auth_3mth_post_acute_cir',\n",
    " 'auth_3mth_post_acute_dia',\n",
    " 'auth_3mth_post_er',\n",
    " 'auth_3mth_dc_no_ref',\n",
    " 'bh_ip_snf_mbr_resp_pmpm_cost_3to6m_b4',\n",
    " 'auth_3mth_acute',\n",
    " 'rx_branded_pmpm_ct_t_6-3-0m_b4',\n",
    " 'atlas_farm_to_school13',\n",
    " 'auth_3mth_acute_cer',\n",
    " 'med_ambulance_coins_pmpm_cost_t_9-6-3m_b4',\n",
    " 'auth_3mth_acute_gus',\n",
    " 'rx_gpi4_6110_pmpm_ct',\n",
    " 'cons_hxwearbl',\n",
    " 'auth_3mth_ltac',\n",
    " 'auth_3mth_acute_ckd',\n",
    " 'bh_ip_snf_net_paid_pmpm_cost_6to9m_b4',\n",
    " 'sex_cd',\n",
    " 'days_since_last_clm_0to3m_b4',\n",
    " 'atlas_perpov_1980_0711',\n",
    " 'auth_3mth_post_acute_mus',\n",
    " 'auth_3mth_non_er',\n",
    " 'bh_ncal_ind',\n",
    " 'auth_3mth_facility',\n",
    " 'atlas_foodinsec_13_15',\n",
    " 'auth_3mth_dc_left_ama',\n",
    " 'race_cd',\n",
    " 'bh_ip_snf_admit_days_pmpm_t_9-6-3m_b4',\n",
    " 'auth_3mth_dc_other',\n",
    " 'cons_stlnindx',\n",
    " 'auth_3mth_acute_skn',\n",
    " 'total_allowed_pmpm_cost_t_9-6-3m_b4',\n",
    " 'auth_3mth_rehab',\n",
    " 'bh_urgent_care_copay_pmpm_cost_t_12-9-6m_b4',\n",
    " 'auth_3mth_dc_custodial',\n",
    " 'auth_3mth_snf_direct',\n",
    " 'auth_3mth_acute_ccs_042',\n",
    " 'bh_ip_snf_net_paid_pmpm_cost_9to12m_b4',\n",
    " 'bh_ip_snf_net_paid_pmpm_cost_3to6m_b4',\n",
    " 'rx_maint_pmpm_cost_t_12-9-6m_b4',\n",
    " 'auth_3mth_post_acute_rsk',\n",
    " 'rev_cms_ansth_pmpm_ct',\n",
    " 'cons_cwht']\n",
    "\n",
    "target = \"covid_vaccination\"\n",
    "\n",
    "student_id = 2000728661\n",
    "\n",
    "id = \"ID\"\n",
    "reg_scalar = MinMaxScaler()\n",
    "reg_f, cat_f = fdf = SelectKBest(f_regression, k=50), SelectKBest(f_classif, k=70)\n",
    "def scale_df(dataframe, train=True):\n",
    "    \n",
    "    if train: dataframe[reg_cols] = reg_scalar.fit_transform(dataframe[reg_cols])\n",
    "    else: dataframe[reg_cols] = reg_scalar.transform(dataframe[reg_cols])\n",
    "    return dataframe\n",
    "\n",
    "def get_reduced_features(dataframe, train=True):\n",
    "    rcat, rreg = [], []\n",
    "    if train: \n",
    "        rcat = cat_f.fit(dataframe[cat_cols], dataframe[target]).get_support(indices=True)\n",
    "        rreg = reg_f.fit(dataframe[reg_cols], dataframe[target]).get_support(indices=True)\n",
    "    else:\n",
    "        rcat = cat_f.get_support(indices=True)\n",
    "        rreg = reg_f.get_support(indices=True)\n",
    "    rcat = [i for idx, i in enumerate(dataframe[cat_cols].columns) if idx in rcat]\n",
    "    rreg = [i for idx, i in enumerate(dataframe[reg_cols].columns) if idx in rreg]\n",
    "    return rcat, rreg\n",
    "\n",
    "df = pd.read_csv('dataset/transformed_dataset.csv')\n",
    "\n",
    "cat_cols, reg_cols = get_reduced_features(df)\n",
    "\n",
    "\n",
    "df = df[cat_cols + reg_cols + [target]]\n",
    "df = scale_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee604de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_T, y_T = df[reg_cols + cat_cols], df[target]\n",
    "# X_T, X_t, y_T, y_t = train_test_split(X, y, test_size=.2, random_state=student_id, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f79420",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth':[9, 10, 12],\n",
    "    'learning_rate': [.1],\n",
    "    'n_estimators': [200, 250]\n",
    "}\n",
    "\n",
    "# m = CatBoostClassifier(random_state = student_id, task_type=\"GPU\", devices='0:1', \n",
    "#                            eval_metric='AUC', thread_count=1, \n",
    "#                            cat_features=cat_cols, metric_period=40,\n",
    "#                            od_type='Iter', loss_function=\"Logloss\", \n",
    "#                        depth=12, learning_rate=learning_rate[1],n_estimators=n_estimators[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5c30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.fit(X_T,y=y_T,eval_set=(X_t, y_t),verbose=True,plot=True, use_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5c0562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, output_process=False):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y, params={'verbose': -1}, free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n",
    "        params = {'application':'binary', 'metric':'auc'}\n",
    "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['max_bin'] = int(round(max_depth))\n",
    "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        params['verbose']: -1\n",
    "        \n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, \n",
    "                           verbose_eval =200, metrics=['auc'], categorical_feature=cat_cols)\n",
    "        return max(cv_result['auc-mean'])\n",
    "     \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n",
    "                                            'num_leaves': (24, 80),\n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'bagging_fraction': (0.8, 1),\n",
    "                                            'max_depth': (5, 30),\n",
    "                                            'max_bin':(20,90),\n",
    "                                            'min_data_in_leaf': (20, 80),\n",
    "                                            'min_sum_hessian_in_leaf':(0,100),\n",
    "                                           'subsample': (0.01, 1.0)\n",
    "                                           }, \n",
    "                                 random_state=student_id)\n",
    "\n",
    "    \n",
    "    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n",
    "    \n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    model_auc=[]\n",
    "    for model in range(len( lgbBO.res)):\n",
    "        model_auc.append(lgbBO.res[model]['target'])\n",
    "    \n",
    "    # return best parameters\n",
    "    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97cfa0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9394553504959928, subsample=0.15203912244729975 will be ignored. Current value: bagging_fraction=0.9394553504959928\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1714\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9394553504959928, subsample=0.15203912244729975 will be ignored. Current value: bagging_fraction=0.9394553504959928\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1714\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9394553504959928, subsample=0.15203912244729975 will be ignored. Current value: bagging_fraction=0.9394553504959928\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030816 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1714\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9394553504959928, subsample=0.15203912244729975 will be ignored. Current value: bagging_fraction=0.9394553504959928\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6539  \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 0.6074  \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 65.42   \u001b[0m | \u001b[0m 11.41   \u001b[0m | \u001b[0m 25.52   \u001b[0m | \u001b[0m 82.24   \u001b[0m | \u001b[0m 58.57   \u001b[0m | \u001b[0m 0.152   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9656700131903726, subsample=0.027180498894965627 will be ignored. Current value: bagging_fraction=0.9656700131903726\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2498\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9656700131903726, subsample=0.027180498894965627 will be ignored. Current value: bagging_fraction=0.9656700131903726\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2498\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9656700131903726, subsample=0.027180498894965627 will be ignored. Current value: bagging_fraction=0.9656700131903726\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2498\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9656700131903726, subsample=0.027180498894965627 will be ignored. Current value: bagging_fraction=0.9656700131903726\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.6571  \u001b[0m | \u001b[95m 0.9657  \u001b[0m | \u001b[95m 0.1381  \u001b[0m | \u001b[95m 0.9864  \u001b[0m | \u001b[95m 39.29   \u001b[0m | \u001b[95m 26.4    \u001b[0m | \u001b[95m 67.65   \u001b[0m | \u001b[95m 28.85   \u001b[0m | \u001b[95m 40.19   \u001b[0m | \u001b[95m 0.02718 \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8528022715071093, subsample=0.07583011969122037 will be ignored. Current value: bagging_fraction=0.8528022715071093\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2088\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8528022715071093, subsample=0.07583011969122037 will be ignored. Current value: bagging_fraction=0.8528022715071093\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2088\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8528022715071093, subsample=0.07583011969122037 will be ignored. Current value: bagging_fraction=0.8528022715071093\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2088\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8528022715071093, subsample=0.07583011969122037 will be ignored. Current value: bagging_fraction=0.8528022715071093\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.6704  \u001b[0m | \u001b[95m 0.8528  \u001b[0m | \u001b[95m 0.2803  \u001b[0m | \u001b[95m 0.2268  \u001b[0m | \u001b[95m 24.09   \u001b[0m | \u001b[95m 18.47   \u001b[0m | \u001b[95m 29.98   \u001b[0m | \u001b[95m 17.46   \u001b[0m | \u001b[95m 49.41   \u001b[0m | \u001b[95m 0.07583 \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8209030603869213, subsample=0.25412142942977417 will be ignored. Current value: bagging_fraction=0.8209030603869213\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8209030603869213, subsample=0.25412142942977417 will be ignored. Current value: bagging_fraction=0.8209030603869213\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8209030603869213, subsample=0.25412142942977417 will be ignored. Current value: bagging_fraction=0.8209030603869213\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8209030603869213, subsample=0.25412142942977417 will be ignored. Current value: bagging_fraction=0.8209030603869213\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.6544  \u001b[0m | \u001b[0m 0.8209  \u001b[0m | \u001b[0m 0.8487  \u001b[0m | \u001b[0m 0.8517  \u001b[0m | \u001b[0m 71.76   \u001b[0m | \u001b[0m 27.36   \u001b[0m | \u001b[0m 57.34   \u001b[0m | \u001b[0m 46.06   \u001b[0m | \u001b[0m 53.96   \u001b[0m | \u001b[0m 0.2541  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8294528638666859, subsample=0.5610635424687966 will be ignored. Current value: bagging_fraction=0.8294528638666859\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1714\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8294528638666859, subsample=0.5610635424687966 will be ignored. Current value: bagging_fraction=0.8294528638666859\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1714\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8294528638666859, subsample=0.5610635424687966 will be ignored. Current value: bagging_fraction=0.8294528638666859\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1714\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8294528638666859, subsample=0.5610635424687966 will be ignored. Current value: bagging_fraction=0.8294528638666859\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6638  \u001b[0m | \u001b[0m 0.8295  \u001b[0m | \u001b[0m 0.1512  \u001b[0m | \u001b[0m 0.5515  \u001b[0m | \u001b[0m 89.18   \u001b[0m | \u001b[0m 11.44   \u001b[0m | \u001b[0m 46.65   \u001b[0m | \u001b[0m 5.227   \u001b[0m | \u001b[0m 58.41   \u001b[0m | \u001b[0m 0.5611  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933691768740116, subsample=0.40841749675474526 will be ignored. Current value: bagging_fraction=0.933691768740116\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1928\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933691768740116, subsample=0.40841749675474526 will be ignored. Current value: bagging_fraction=0.933691768740116\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1928\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933691768740116, subsample=0.40841749675474526 will be ignored. Current value: bagging_fraction=0.933691768740116\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1928\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.933691768740116, subsample=0.40841749675474526 will be ignored. Current value: bagging_fraction=0.933691768740116\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6625  \u001b[0m | \u001b[0m 0.9337  \u001b[0m | \u001b[0m 0.3663  \u001b[0m | \u001b[0m 0.6223  \u001b[0m | \u001b[0m 21.31   \u001b[0m | \u001b[0m 14.95   \u001b[0m | \u001b[0m 26.44   \u001b[0m | \u001b[0m 12.65   \u001b[0m | \u001b[0m 50.33   \u001b[0m | \u001b[0m 0.4084  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8156522390207586, subsample=0.9363048836317135 will be ignored. Current value: bagging_fraction=0.8156522390207586\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2036\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8156522390207586, subsample=0.9363048836317135 will be ignored. Current value: bagging_fraction=0.8156522390207586\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2036\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8156522390207586, subsample=0.9363048836317135 will be ignored. Current value: bagging_fraction=0.8156522390207586\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2036\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8156522390207586, subsample=0.9363048836317135 will be ignored. Current value: bagging_fraction=0.8156522390207586\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.6562  \u001b[0m | \u001b[0m 0.8157  \u001b[0m | \u001b[0m 0.6496  \u001b[0m | \u001b[0m 0.9015  \u001b[0m | \u001b[0m 23.03   \u001b[0m | \u001b[0m 17.41   \u001b[0m | \u001b[0m 27.31   \u001b[0m | \u001b[0m 11.67   \u001b[0m | \u001b[0m 46.89   \u001b[0m | \u001b[0m 0.9363  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.870598517924377, subsample=0.10354251376827969 will be ignored. Current value: bagging_fraction=0.870598517924377\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2447\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.870598517924377, subsample=0.10354251376827969 will be ignored. Current value: bagging_fraction=0.870598517924377\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2447\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.870598517924377, subsample=0.10354251376827969 will be ignored. Current value: bagging_fraction=0.870598517924377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2447\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.870598517924377, subsample=0.10354251376827969 will be ignored. Current value: bagging_fraction=0.870598517924377\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.6706  \u001b[0m | \u001b[95m 0.8706  \u001b[0m | \u001b[95m 0.8648  \u001b[0m | \u001b[95m 0.1456  \u001b[0m | \u001b[95m 65.37   \u001b[0m | \u001b[95m 24.58   \u001b[0m | \u001b[95m 72.78   \u001b[0m | \u001b[95m 82.93   \u001b[0m | \u001b[95m 40.56   \u001b[0m | \u001b[95m 0.1035  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8438071822220923, subsample=0.8660473018279127 will be ignored. Current value: bagging_fraction=0.8438071822220923\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1542\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8438071822220923, subsample=0.8660473018279127 will be ignored. Current value: bagging_fraction=0.8438071822220923\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1542\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8438071822220923, subsample=0.8660473018279127 will be ignored. Current value: bagging_fraction=0.8438071822220923\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1542\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8438071822220923, subsample=0.8660473018279127 will be ignored. Current value: bagging_fraction=0.8438071822220923\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6572  \u001b[0m | \u001b[0m 0.8438  \u001b[0m | \u001b[0m 0.5557  \u001b[0m | \u001b[0m 0.6751  \u001b[0m | \u001b[0m 80.11   \u001b[0m | \u001b[0m 7.722   \u001b[0m | \u001b[0m 56.83   \u001b[0m | \u001b[0m 21.84   \u001b[0m | \u001b[0m 72.31   \u001b[0m | \u001b[0m 0.866   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8888305827943862, subsample=0.2836990781091962 will be ignored. Current value: bagging_fraction=0.8888305827943862\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2343\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8888305827943862, subsample=0.2836990781091962 will be ignored. Current value: bagging_fraction=0.8888305827943862\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2343\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8888305827943862, subsample=0.2836990781091962 will be ignored. Current value: bagging_fraction=0.8888305827943862\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2343\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8888305827943862, subsample=0.2836990781091962 will be ignored. Current value: bagging_fraction=0.8888305827943862\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.658   \u001b[0m | \u001b[0m 0.8888  \u001b[0m | \u001b[0m 0.6983  \u001b[0m | \u001b[0m 0.7174  \u001b[0m | \u001b[0m 74.12   \u001b[0m | \u001b[0m 23.21   \u001b[0m | \u001b[0m 41.03   \u001b[0m | \u001b[0m 27.92   \u001b[0m | \u001b[0m 54.88   \u001b[0m | \u001b[0m 0.2837  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8704260618926223, subsample=0.3484445232992769 will be ignored. Current value: bagging_fraction=0.8704260618926223\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1487\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8704260618926223, subsample=0.3484445232992769 will be ignored. Current value: bagging_fraction=0.8704260618926223\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1487\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8704260618926223, subsample=0.3484445232992769 will be ignored. Current value: bagging_fraction=0.8704260618926223\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1487\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8704260618926223, subsample=0.3484445232992769 will be ignored. Current value: bagging_fraction=0.8704260618926223\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6638  \u001b[0m | \u001b[0m 0.8704  \u001b[0m | \u001b[0m 0.5722  \u001b[0m | \u001b[0m 0.5164  \u001b[0m | \u001b[0m 56.84   \u001b[0m | \u001b[0m 7.312   \u001b[0m | \u001b[0m 42.41   \u001b[0m | \u001b[0m 1.643   \u001b[0m | \u001b[0m 34.7    \u001b[0m | \u001b[0m 0.3484  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1825\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1825\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1825\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m 0.6735  \u001b[0m | \u001b[95m 0.8557  \u001b[0m | \u001b[95m 0.1826  \u001b[0m | \u001b[95m 0.08423 \u001b[0m | \u001b[95m 81.58   \u001b[0m | \u001b[95m 13.12   \u001b[0m | \u001b[95m 53.53   \u001b[0m | \u001b[95m 10.12   \u001b[0m | \u001b[95m 44.97   \u001b[0m | \u001b[95m 0.4213  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8417108501405114, subsample=0.8195802841787828 will be ignored. Current value: bagging_fraction=0.8417108501405114\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1714\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8417108501405114, subsample=0.8195802841787828 will be ignored. Current value: bagging_fraction=0.8417108501405114\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1714\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8417108501405114, subsample=0.8195802841787828 will be ignored. Current value: bagging_fraction=0.8417108501405114\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1714\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8417108501405114, subsample=0.8195802841787828 will be ignored. Current value: bagging_fraction=0.8417108501405114\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.654   \u001b[0m | \u001b[0m 0.8417  \u001b[0m | \u001b[0m 0.7035  \u001b[0m | \u001b[0m 0.9161  \u001b[0m | \u001b[0m 60.85   \u001b[0m | \u001b[0m 11.09   \u001b[0m | \u001b[0m 22.75   \u001b[0m | \u001b[0m 31.03   \u001b[0m | \u001b[0m 59.12   \u001b[0m | \u001b[0m 0.8196  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9319399790207467, subsample=0.28078188621189815 will be ignored. Current value: bagging_fraction=0.9319399790207467\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2192\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9319399790207467, subsample=0.28078188621189815 will be ignored. Current value: bagging_fraction=0.9319399790207467\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2192\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9319399790207467, subsample=0.28078188621189815 will be ignored. Current value: bagging_fraction=0.9319399790207467\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2192\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9319399790207467, subsample=0.28078188621189815 will be ignored. Current value: bagging_fraction=0.9319399790207467\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6683  \u001b[0m | \u001b[0m 0.9319  \u001b[0m | \u001b[0m 0.646   \u001b[0m | \u001b[0m 0.156   \u001b[0m | \u001b[0m 22.9    \u001b[0m | \u001b[0m 20.08   \u001b[0m | \u001b[0m 30.38   \u001b[0m | \u001b[0m 28.69   \u001b[0m | \u001b[0m 77.68   \u001b[0m | \u001b[0m 0.2808  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8169079761880753, subsample=0.7143669405054213 will be ignored. Current value: bagging_fraction=0.8169079761880753\n",
      "[LightGBM] [Info] Number of positive: 112968, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2447\n",
      "[LightGBM] [Info] Number of data points in the train set: 649894, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8169079761880753, subsample=0.7143669405054213 will be ignored. Current value: bagging_fraction=0.8169079761880753\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2447\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8169079761880753, subsample=0.7143669405054213 will be ignored. Current value: bagging_fraction=0.8169079761880753\n",
      "[LightGBM] [Info] Number of positive: 112969, number of negative: 536926\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2447\n",
      "[LightGBM] [Info] Number of data points in the train set: 649895, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8169079761880753, subsample=0.7143669405054213 will be ignored. Current value: bagging_fraction=0.8169079761880753\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6662  \u001b[0m | \u001b[0m 0.8169  \u001b[0m | \u001b[0m 0.4682  \u001b[0m | \u001b[0m 0.2626  \u001b[0m | \u001b[0m 75.16   \u001b[0m | \u001b[0m 25.28   \u001b[0m | \u001b[0m 21.57   \u001b[0m | \u001b[0m 46.05   \u001b[0m | \u001b[0m 79.35   \u001b[0m | \u001b[0m 0.7144  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "opt_params = bayes_parameter_opt_lgb(X_T, y_T, init_round=5, opt_round=10, n_folds=3, random_seed=student_id,n_estimators=10000)\n",
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='binary'\n",
    "opt_params[1]['metric']='auc'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['boost_from_average']=False\n",
    "opt_params=opt_params[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "381141cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8557273995250181,\n",
       " 'feature_fraction': 0.1826149206958128,\n",
       " 'learning_rate': 0.08422530767164066,\n",
       " 'max_bin': 82,\n",
       " 'max_depth': 13,\n",
       " 'min_data_in_leaf': 54,\n",
       " 'min_sum_hessian_in_leaf': 10.122800844675272,\n",
       " 'num_leaves': 45,\n",
       " 'subsample': 0.42126292393562287,\n",
       " 'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'is_unbalance': True,\n",
       " 'boost_from_average': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "550fcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# m = CatBoostClassifier(random_state = student_id, task_type=\"GPU\", devices='0:1', \n",
    "#                            eval_metric='AUC', thread_count=1, \n",
    "#                            cat_features=cat_cols, metric_period=40,\n",
    "#                            od_type='Iter', loss_function=\"Logloss\",\n",
    "#                        depth=9, learning_rate=.1,n_estimators=200)\n",
    "\n",
    "# # best_params = m.grid_search(params, X,y=y, plot=True, stratified=True, cv=4, verbose=True)\n",
    "# m.fit(X, y=y, plot=True, )\n",
    "\n",
    "# m.save_model('models/catboost.cbm',\n",
    "#            format=\"cbm\",\n",
    "#            export_parameters=None,\n",
    "#            pool=None)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f49fb0d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adec86b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 152507, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5317\n",
      "[LightGBM] [Info] Number of data points in the train set: 877357, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[181]\ttraining's auc: 0.716688\tvalid_1's auc: 0.675426\n",
      "Fold 1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 152507, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5310\n",
      "[LightGBM] [Info] Number of data points in the train set: 877357, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[157]\ttraining's auc: 0.711809\tvalid_1's auc: 0.673459\n",
      "Fold 2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5312\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[172]\ttraining's auc: 0.714409\tvalid_1's auc: 0.675159\n",
      "Fold 3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5304\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttraining's auc: 0.724498\tvalid_1's auc: 0.675538\n",
      "Fold 4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5312\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[220]\ttraining's auc: 0.723147\tvalid_1's auc: 0.675922\n",
      "Fold 5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5313\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[213]\ttraining's auc: 0.722362\tvalid_1's auc: 0.676013\n",
      "Fold 6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5310\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[125]\ttraining's auc: 0.704215\tvalid_1's auc: 0.675027\n",
      "Fold 7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5316\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "Training until validation scores don't improve for 250 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[218]\ttraining's auc: 0.722623\tvalid_1's auc: 0.677651\n",
      "Fold 8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 152508, number of negative: 724850\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5310\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[130]\ttraining's auc: 0.70564\tvalid_1's auc: 0.674811\n",
      "Fold 9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "[LightGBM] [Info] Number of positive: 152507, number of negative: 724851\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5316\n",
      "[LightGBM] [Info] Number of data points in the train set: 877358, number of used features: 120\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8557273995250181, subsample=0.42126292393562287 will be ignored. Current value: bagging_fraction=0.8557273995250181\n",
      "Training until validation scores don't improve for 250 rounds\n",
      "Early stopping, best iteration is:\n",
      "[217]\ttraining's auc: 0.722549\tvalid_1's auc: 0.675768\n"
     ]
    }
   ],
   "source": [
    "tdf = pd.read_csv('dataset/transformed_dataset_holdout.csv')\n",
    "tdf = tdf[reg_cols + cat_cols + [id]]\n",
    "tdf = scale_df(tdf, False)\n",
    "\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=31416)\n",
    "oof = np.zeros(len(X_T))\n",
    "predictions = np.zeros(len(tdf))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "features = reg_cols + cat_cols\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_T.values, y_T.values)):\n",
    "    print(\"Fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(X_T.iloc[trn_idx][features], label=y_T.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(X_T.iloc[val_idx][features], label=y_T.iloc[val_idx])\n",
    "\n",
    "    num_round = 15000\n",
    "    clf = lgb.train(opt_params, trn_data, num_round, valid_sets = [trn_data, val_data], \n",
    "                    verbose_eval=500, early_stopping_rounds = 250, categorical_feature=cat_cols)\n",
    "    oof[val_idx] = clf.predict(X_T.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(tdf[features], num_iteration=clf.best_iteration) / folds.n_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2cff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((974842,),)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e390401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.67546 \n"
     ]
    }
   ],
   "source": [
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(y_T, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a805c9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.35826804, 0.18942948, 0.52470684, ..., 0.64224874, 0.43189335,\n",
       "        0.64363892]),\n",
       " numpy.ndarray)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ebbcf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tdf[target] = predictions > .5\n",
    "tdf['SCORE'] = 1. - predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d95f8cde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tdf['RANK'] = tdf['SCORE'].rank(ascending=False, method='first').astype(np.int64)\n",
    "tdf['SCORE'] = tdf['SCORE'].round(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a13b0b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    805389\n",
       "1    169453\n",
       "Name: covid_vaccination, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_T.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4820b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_csv('dataset/2021CaseCompetition_Ashutosh_Tiwari_20211006.csv', index=False, columns=[id, 'SCORE', 'RANK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2451105c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525158, 124)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "915ff73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    283235\n",
       "True     241923\n",
       "Name: covid_vaccination, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5322bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
